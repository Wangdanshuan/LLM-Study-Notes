#                                  能读到此篇的就算是有缘人，这绝对是心血之作 
# 1.问题背景
当使用LORA技术微调完CHATGLM后，官方给出的推断方案是`peft.AutoPeftModelForCausalLM`加载模型，调用`chat`方法进行推断。
它的好处是简便，无需任何的数据处理，输入原始文本即可得到输出。
但是问题是效率太低，它仅支持单个样本的输入。他在内部需要实现自然语言与token id的编码接码相关处理，这也是降低效率的因素。
# 2.解决方案
## 2.1 对量化模型进行LORA微调
## 2.2 使用更底层的接口，而非chat

通过上述方案，我实现了单张A100每秒推断10个样本，样本平均长度2000 tokens. 效率提升约200倍

感兴趣可联系本文作者： 903012463@qq.com
